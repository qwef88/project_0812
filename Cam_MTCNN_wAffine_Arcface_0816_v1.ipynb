{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ArcFace 실습에 필요한 파이썬 라이브러리를 불러옵니다.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch # pytorch의 tensor와 그와 관련된 기본 연산 등을 지원\n",
    "import torch.nn as nn # 여러 딥러닝 layer와 loss, 함수 등을 클래스 형태로 지원\n",
    "import torch.nn.functional as F # 여러 loss, 함수 등을 function 형태로 지원\n",
    "import torch.optim as optim # 여러 optimizer를 지원\n",
    "from torch.utils.data import DataLoader # 데이터 로더를 쉽게 만들 수 있도록 지원\n",
    "\n",
    "import torchvision.transforms as T # 이미지 전처리를 지원\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils # 여러가지 편리한 기능을 지원 (ex. grid 이미지 만들기 등)\n",
    "import torchvision.models as models # VGG, ResNet 등을 바로 로드할 수 있도록 지원\n",
    "from torchvision.datasets import ImageFolder # (img, label) 형태의 데이터셋 구성을 쉽게 할 수 있도록 지원\n",
    "\n",
    "from AffineTransform import AffineTransform\n",
    "from mtcnn import detect_faces\n",
    "\n",
    "# GPU 선택\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "# GPU가 있을 경우 연산을 GPU에서 하고 없을 경우 CPU에서 진행\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    '''\n",
    "    목적 : Arc marin 을 포함한 last fc layer의 구현\n",
    "    \n",
    "    인자 :\n",
    "    in_features : feature의 dimension\n",
    "    out_features : class 개수\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        # fc의 parameter 만들기\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        '''\n",
    "        Step 1. cos(theta + m) 계산하기\n",
    "        '''\n",
    "        # cosine : cos(theta)        \n",
    "        # linear(x, W) = x * W^T = [N, in_features] * [in_features, out_features] = [N, out_features]\n",
    "        # [N, out_features] / normalize-> cos dist == cos\n",
    "        # F.normalize는 디폴트가 dim = 1 (줄이고자하는 dimension)\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        \n",
    "        # c^2 + s^2 = 1 \n",
    "        sine = torch.sqrt((1.00000001 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        \n",
    "        # cos(theta + m) = cos(theta) * cos(m) - sin(theta) * sin(m)\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        '''\n",
    "        Step 2. cos(theta + m) 에서 dim=1에 대해 y_i에 해당하는 부분만 남기고 나머지는 cos(theta)로 되돌리기 \n",
    "        '''\n",
    "        one_hot = torch.zeros(cosine.size()).to(dev)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        '''\n",
    "        Step 3. 이 output이 softmax -> cross entropy loss로 흘러가면 된다.\n",
    "        '''\n",
    "        return output\n",
    "\n",
    "class FeatureNet_50(nn.Module):\n",
    "    '''\n",
    "    목적 : ResNet-50을 이용한 backbone network(feature extractor) 구현 \n",
    "    \n",
    "    인자 :\n",
    "    feature_dim : feature의 dimension\n",
    "    gray_scale : 이미지를 gray scale로 받았는지 여부\n",
    "    '''\n",
    "    def __init__(self, feature_dim, gray_scale=True):\n",
    "        super(FeatureNet_50, self).__init__()\n",
    "        # Pytorch에서 이미 구현되어 있는 resnet-50 불러오기\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 이런식으로 불러온 resnet을 조건에 맞게 변경할 수 있다.\n",
    "        if gray_scale:\n",
    "            resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            \n",
    "        # resnet의 마지막 conv block까지만 남기고 나머지 부분 잘라내기\n",
    "        self.backbone = nn.Sequential(* list(resnet.children())[0:-2])\n",
    "        \n",
    "        # resnet의 마지막 conv block 뒤쪽으로 새로 붙을 layer 들\n",
    "        self.bn_4 = nn.BatchNorm2d(2048)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(2048 * 4 * 4, feature_dim)\n",
    "        self.bn_5 = nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.bn_4(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # FC layer를 지나기 전에는 reshape 과정이 필요하다.\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.bn_5(out)\n",
    "        return out\n",
    "\n",
    "class ArcFaceNet(nn.Module):\n",
    "    '''\n",
    "    ArcMarginProduct와 FeatureNet-50 을 결합한 ArcFace 모델의 구현\n",
    "    '''\n",
    "    def __init__(self, feature_dim, cls_num, gray_scale=True):\n",
    "        super(ArcFaceNet, self).__init__()\n",
    "        self.feature_net = FeatureNet_50(feature_dim, gray_scale=gray_scale)\n",
    "        self.classifier = ArcMarginProduct(feature_dim, cls_num)\n",
    "\n",
    "    # 끝까지 Forward 하여 logit을 return\n",
    "    def forward(self, x, label):\n",
    "        out = self.feature_net(x)\n",
    "        out = self.classifier(out, label)\n",
    "        return out\n",
    "    \n",
    "    # Feature만 return\n",
    "    def extract_feature(self, x):\n",
    "        out = self.feature_net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 Hyperparameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 경로\n",
    "trn_data_dir  = './data/CASIA-WebFace'\n",
    "tst_data_dir  = './data/lfw'\n",
    "lfw_pair_text = 'lfw_test_part.txt'\n",
    "log_dir       = 'log/arcface'\n",
    "weight_dir    = 'weights/arcface'\n",
    "\n",
    "# 데이터 관련 세팅\n",
    "batch_size = 16\n",
    "resize = 128\n",
    "crop = 128\n",
    "cls_num=10575\n",
    "gray_scale = True\n",
    "\n",
    "# Hyperparameter\n",
    "feature_dim = 512\n",
    "lr = 0.1  # initial learning rate\n",
    "lr_step = 10000\n",
    "decay_ratio = 0.1\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Setting\n",
    "max_epoch = 200\n",
    "save_every = 10000\n",
    "record_every = 1000\n",
    "display_every = 1000\n",
    "test_num = 6000\n",
    "\n",
    "\n",
    "# 디렉토리가 없을 시 생성\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 객체들을 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크\n",
    "net = ArcFaceNet(feature_dim=feature_dim, \n",
    "                 cls_num=cls_num, \n",
    "                 gray_scale=gray_scale).to(dev)\n",
    "\n",
    "# optimizer 및 learning rate scheduler\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=decay_ratio)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_similarity(features, test_feature):\n",
    "    sum_ = torch.mm(features, torch.t(test_feature)).squeeze()\n",
    "    norm = torch.norm(features, dim=1) * torch.norm(test_feature)\n",
    "    return sum_ / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image):\n",
    "    trans_list = []        \n",
    "    trans_list += [T.Grayscale(num_output_channels=1),\n",
    "                   T.Resize((128, 128)),\n",
    "                   T.ToTensor(),\n",
    "                   T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    \n",
    "    transformer = T.Compose(trans_list)\n",
    "    \n",
    "    cropped_image = transformer(image)\n",
    "    \n",
    "    return cropped_image.unsqueeze(0).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "load_iters = 280000\n",
    "ckpt_name = 'ckpt_' + str(load_iters) + '.pkl'\n",
    "ckpt_path = os.path.join('weights/', ckpt_name)\n",
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(ckpt_path))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(ckpt_path,map_location='cpu'))\n",
    "    \n",
    "net.eval()\n",
    "torch.no_grad()\n",
    "\n",
    "transform = AffineTransform()\n",
    "\n",
    "f=open(\"DB.txt\")\n",
    "tmp=f.read()\n",
    "tmp=tmp.split(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "known_face_names = []\n",
    "for i in range(len(tmp)):\n",
    "    if len(tmp[i])>0:\n",
    "        data=tmp[i].split()\n",
    "        known_face_names.append(data[0])\n",
    "        \n",
    "        cv2_im = cv2.imread(data[1])\n",
    "        pil_im = Image.open(data[1])\n",
    "        \n",
    "        bonding_boxes, landmark_points = detect_faces(pil_im)\n",
    "        aligned_images = transform.extract_image_chips(img = cv2_im, points = landmark_points, desired_size=128, padding=0.1)\n",
    "        \n",
    "        img=Image.fromarray(aligned_images[0])\n",
    "        image_cropped = crop_image(img)\n",
    "        face_feature = net.extract_feature(image_cropped.to(dev))\n",
    "        \n",
    "        if i==0:\n",
    "            known_face_features=torch.tensor(face_feature)\n",
    "        else:\n",
    "            known_face_features=torch.cat((known_face_features, face_feature))\n",
    "        \n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    _, frame = video_capture.read()\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    pil_im = Image.fromarray(cv2_im)\n",
    "    \n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    bounding_boxes, landmark_points = detect_faces(pil_im)\n",
    "    aligned_images = transform.extract_image_chips(frame, points = landmark_points, desired_size=128, padding=0.1)\n",
    "\n",
    "    for i in range(len(aligned_images)):\n",
    "        box=bounding_boxes[i]\n",
    "        cv2_img = cv2.cvtColor(aligned_images[i],cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(cv2_img)\n",
    "        cropped_image = crop_image(pil_img)\n",
    "        unknown_face_feature=net.extract_feature(cropped_image)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Or instead, use the known face with the smallest distance to the new face\n",
    "        distances = face_similarity(known_face_features, unknown_face_feature)\n",
    "        #print(distances)\n",
    "        best_match_index = torch.argmax(distances)\n",
    "        if torch.max(distances)>0.3:\n",
    "            name = known_face_names[best_match_index]\n",
    "\n",
    "        (top, right, bottom, left) = box\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        cv2.putText(frame, str(\"Sim={0:.2f}>0.3\".format(torch.max(distances))), (left + 6, bottom + 25), font, 0.7, (255, 255, 255), 1)\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
