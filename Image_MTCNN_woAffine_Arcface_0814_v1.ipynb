{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ArcFace 실습에 필요한 파이썬 라이브러리를 불러옵니다.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch # pytorch의 tensor와 그와 관련된 기본 연산 등을 지원\n",
    "import torch.nn as nn # 여러 딥러닝 layer와 loss, 함수 등을 클래스 형태로 지원\n",
    "import torch.nn.functional as F # 여러 loss, 함수 등을 function 형태로 지원\n",
    "import torch.optim as optim # 여러 optimizer를 지원\n",
    "\n",
    "from torchvision.datasets import ImageFolder # (img, label) 형태의 데이터셋 구성을 쉽게 할 수 있도록 지원\n",
    "import torchvision.transforms as T # 이미지 전처리를 지원\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils # 여러가지 편리한 기능을 지원 (ex. grid 이미지 만들기 등)\n",
    "import torchvision.models as models # VGG, ResNet 등을 바로 로드할 수 있도록 지원\n",
    "from torch.utils.data import DataLoader # 데이터 로더를 쉽게 만들 수 있도록 지원\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # Tensorflow의 Tensorboard를 지원\n",
    "\n",
    "# GPU 선택\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training에 사용될 데이터를 불러옵니다.\n",
    "  \n",
    "이번 실습에서 사용할 데이터셋은 __CASIA-Webface__ 데이터셋입니다.\n",
    "  \n",
    "  \n",
    "- 총 ID 개수 : 10575  \n",
    "- 총 이미지 개수 : 494414\n",
    "- ID 당 최소 이미지 개수 : 2\n",
    "- ID 당 최대 이미지 개수 : 804\n",
    "- ID 당 평균 이미지 개수 : 46.75\n",
    "- 이미지 크기 : 250 x 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(root, batch_size, resize, crop, gray_scale, shuffle):\n",
    "    '''\n",
    "    목적 : (image, label) 을 반복적으로 로드해주는 데이터 로더 만들기\n",
    "    \n",
    "    인자:\n",
    "    root : 이미지의 루트 디렉토리\n",
    "    batck_size : 배치 사이즈\n",
    "    resize : 이미지 resize 사이즈\n",
    "    crop : 이미지 crop 사이즈\n",
    "    gray_scale : 흑백 변환 여부\n",
    "    shuffle : 무작위 섞음 여부\n",
    "    '''\n",
    "    \n",
    "    ''' Step 1. 이미지 전처리 '''\n",
    "    trans_list = []\n",
    "    \n",
    "    if gray_scale:\n",
    "        trans_list += [T.Grayscale(num_output_channels=1)]\n",
    "        \n",
    "    trans_list += [T.Resize((resize, resize)),\n",
    "                   T.RandomCrop((crop, crop)),\n",
    "                   T.ToTensor()]\n",
    "    \n",
    "    if gray_scale:\n",
    "        trans_list += [T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    else:\n",
    "        trans_list += [T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]\n",
    "        \n",
    "    # Compose에 원하는 전처리의 list를 전달한다.\n",
    "    transformer = T.Compose(trans_list)\n",
    "    \n",
    "    ''' Step 2. Dataset 구성하기 '''\n",
    "    # root - class directory - images 구조에서 (img, label) 꼴을 로드하는 데이터 로더의 구현에 사용 가능\n",
    "    dataset = ImageFolder(root, transform=transformer)\n",
    "    \n",
    "    ''' Step 3. Data loader 만들기'''\n",
    "    dloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    # Tip : 여기서 data loader 뿐만 아니라 class의 개수, data 전체 개수를 같이 반환하면 나중에 편하다.\n",
    "    return dloader, len(dataset.classes), len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace의 모델을 구현합니다.\n",
    "  \n",
    "- Backbone model은 __ResNet-50__ 모델을 사용하도록 하겠습니다.\n",
    "  \n",
    "  \n",
    "# 절차\n",
    "1. ArcFace의 핵심에 해당하는 마지막 fc 의 class 구현\n",
    "2. Pytorch에서 제공해주는 ResNet 을 불러와서 필요한 부분 변경하기\n",
    "3. 1과 2에서 정의된 모듈을 사용하여 ArcFace network 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    '''\n",
    "    목적 : Arc marin 을 포함한 last fc layer의 구현\n",
    "    \n",
    "    인자 :\n",
    "    in_features : feature의 dimension\n",
    "    out_features : class 개수\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        # fc의 parameter 만들기\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        '''\n",
    "        Step 1. cos(theta + m) 계산하기\n",
    "        '''\n",
    "        # cosine : cos(theta)        \n",
    "        # linear(x, W) = x * W^T = [N, in_features] * [in_features, out_features] = [N, out_features]\n",
    "        # [N, out_features] / normalize-> cos dist == cos\n",
    "        # F.normalize는 디폴트가 dim = 1 (줄이고자하는 dimension)\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        \n",
    "        # c^2 + s^2 = 1 \n",
    "        sine = torch.sqrt((1.00000001 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        \n",
    "        # cos(theta + m) = cos(theta) * cos(m) - sin(theta) * sin(m)\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        '''\n",
    "        Step 2. cos(theta + m) 에서 dim=1에 대해 y_i에 해당하는 부분만 남기고 나머지는 cos(theta)로 되돌리기 \n",
    "        '''\n",
    "        one_hot = torch.zeros(cosine.size()).to(dev)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        '''\n",
    "        Step 3. 이 output이 softmax -> cross entropy loss로 흘러가면 된다.\n",
    "        '''\n",
    "        return output\n",
    "\n",
    "class FeatureNet_50(nn.Module):\n",
    "    '''\n",
    "    목적 : ResNet-50을 이용한 backbone network(feature extractor) 구현 \n",
    "    \n",
    "    인자 :\n",
    "    feature_dim : feature의 dimension\n",
    "    gray_scale : 이미지를 gray scale로 받았는지 여부\n",
    "    '''\n",
    "    def __init__(self, feature_dim, gray_scale=True):\n",
    "        super(FeatureNet_50, self).__init__()\n",
    "        # Pytorch에서 이미 구현되어 있는 resnet-50 불러오기\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 이런식으로 불러온 resnet을 조건에 맞게 변경할 수 있다.\n",
    "        if gray_scale:\n",
    "            resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            \n",
    "        # resnet의 마지막 conv block까지만 남기고 나머지 부분 잘라내기\n",
    "        self.backbone = nn.Sequential(* list(resnet.children())[0:-2])\n",
    "        \n",
    "        # resnet의 마지막 conv block 뒤쪽으로 새로 붙을 layer 들\n",
    "        self.bn_4 = nn.BatchNorm2d(2048)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(2048 * 4 * 4, feature_dim)\n",
    "        self.bn_5 = nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.bn_4(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # FC layer를 지나기 전에는 reshape 과정이 필요하다.\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.bn_5(out)\n",
    "        return out\n",
    "\n",
    "class ArcFaceNet(nn.Module):\n",
    "    '''\n",
    "    ArcMarginProduct와 FeatureNet-50 을 결합한 ArcFace 모델의 구현\n",
    "    '''\n",
    "    def __init__(self, feature_dim, cls_num, gray_scale=True):\n",
    "        super(ArcFaceNet, self).__init__()\n",
    "        self.feature_net = FeatureNet_50(feature_dim, gray_scale=gray_scale)\n",
    "        self.classifier = ArcMarginProduct(feature_dim, cls_num)\n",
    "\n",
    "    # 끝까지 Forward 하여 logit을 return\n",
    "    def forward(self, x, label):\n",
    "        out = self.feature_net(x)\n",
    "        out = self.classifier(out, label)\n",
    "        return out\n",
    "    \n",
    "    # Feature만 return\n",
    "    def extract_feature(self, x):\n",
    "        out = self.feature_net(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 사용될 유틸리티 함수들을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(x1, x2):\n",
    "    '''\n",
    "    목적 : 벡터 x1, x2 사이의 Cosine similary를 계산\n",
    "    \n",
    "    인자 :\n",
    "    x1, x2 : 두 벡터\n",
    "    '''\n",
    "    return torch.sum(x1 * x2) / (torch.norm(x1) * torch.norm(x2))\n",
    "\n",
    "def fixed_img_list(lfw_pair_text, test_num):\n",
    "    '''\n",
    "    목적 : 중간 테스트 때 계속 사용될 고정 test 이미지 리스트 생성\n",
    "    \n",
    "    인자:\n",
    "    lfw_pair_text : pair가 만들어진 이미지 경로\n",
    "    test_num : 테스트에 사용할 pair 개수\n",
    "    '''\n",
    "    f = open(lfw_pair_text, 'r')\n",
    "    lines = []\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        lines.append(line)\n",
    "    f.close()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "    lines = lines[:test_num]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from box_utils import nms, calibrate_box, get_image_boxes, convert_to_square, _preprocess\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, c, h, w].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch_size, c*h*w].\n",
    "        \"\"\"\n",
    "\n",
    "        # without this pretrained model isn't working\n",
    "        x = x.transpose(3, 2).contiguous()\n",
    "\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class PNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "        # suppose we have input with size HxW, then\n",
    "        # after first layer: H - 2,\n",
    "        # after pool: ceil((H - 2)/2),\n",
    "        # after second conv: ceil((H - 2)/2) - 2,\n",
    "        # after last conv: ceil((H - 2)/2) - 4,\n",
    "        # and the same for W\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 10, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(10)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(10, 16, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(16)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(16, 32, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(32))\n",
    "        ]))\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
    "\n",
    "        weights = np.load('weights/pnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
    "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv4_1(x)\n",
    "        b = self.conv4_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class RNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 28, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(28)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(28, 48, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(48)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(48, 64, 2, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv4', nn.Linear(576, 128)),\n",
    "            ('prelu4', nn.PReLU(128))\n",
    "        ]))\n",
    "\n",
    "        self.conv5_1 = nn.Linear(128, 2)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        weights = np.load('weights/rnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv5_1(x)\n",
    "        b = self.conv5_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class ONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(32)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(64)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
    "            ('prelu4', nn.PReLU(128)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv5', nn.Linear(1152, 256)),\n",
    "            ('drop5', nn.Dropout(0.25)),\n",
    "            ('prelu5', nn.PReLU(256)),\n",
    "        ]))\n",
    "\n",
    "        self.conv6_1 = nn.Linear(256, 2)\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "\n",
    "        weights = np.load('weights/onet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            c: a float tensor with shape [batch_size, 10].\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv6_1(x)\n",
    "        b = self.conv6_2(x) # Bounding Box Regression\n",
    "        c = self.conv6_3(x) # Face Landmark Detection\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return c, b, a\n",
    "\n",
    "\n",
    "def run_first_stage(image, net, scale, threshold):\n",
    "    \"\"\"Run P-Net, generate bounding boxes, and do NMS.\n",
    "\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        net: an instance of pytorch's nn.Module, P-Net.\n",
    "        scale: a float number,\n",
    "            scale width and height of the image by this number.\n",
    "        threshold: a float number,\n",
    "            threshold on the probability of a face when generating\n",
    "            bounding boxes from predictions of the net.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9],\n",
    "            bounding boxes with scores and offsets (4 + 1 + 4).\n",
    "    \"\"\"\n",
    "\n",
    "    # scale the image and convert it to a float array\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n",
    "    img = image.resize((int(sw), int(sh)), Image.BILINEAR)\n",
    "    img = np.asarray(img, 'float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = Variable(torch.FloatTensor(_preprocess(img)))\n",
    "#         if torch.cuda.is_available():\n",
    "#             img = img.cuda()\n",
    "#             net = net.cuda()\n",
    "        img=img.cpu()\n",
    "        net=net.cpu()\n",
    "    output = net(img)\n",
    "    probs = output[1].data.cpu().numpy()[0, 1, :, :]\n",
    "    offsets = output[0].data.cpu().numpy()\n",
    "    # probs: probability of a face at each sliding window\n",
    "    # offsets: transformations to true bounding boxes\n",
    "\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \"\"\"Generate bounding boxes at places\n",
    "    where there is probably a face.\n",
    "\n",
    "    Arguments:\n",
    "        probs: a float numpy array of shape [n, m].\n",
    "        offsets: a float numpy array of shape [1, 4, n, m].\n",
    "        scale: a float number,\n",
    "            width and height of the image were scaled by this number.\n",
    "        threshold: a float number.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9]\n",
    "    \"\"\"\n",
    "\n",
    "    # applying P-Net is equivalent, in some sense, to\n",
    "    # moving 12x12 window with stride 2\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    # indices of boxes where there is probably a face\n",
    "    inds = np.where(probs > threshold)\n",
    "\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # transformations of bounding boxes\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    # they are defined as:\n",
    "    # w = x2 - x1 + 1\n",
    "    # h = y2 - y1 + 1\n",
    "    # x1_true = x1 + tx1*w\n",
    "    # x2_true = x2 + tx2*w\n",
    "    # y1_true = y1 + ty1*h\n",
    "    # y2_true = y2 + ty2*h\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "\n",
    "    # P-Net is applied to scaled images\n",
    "    # so we need to rescale bounding boxes back\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride*inds[1] + 1.0)/scale),\n",
    "        np.round((stride*inds[0] + 1.0)/scale),\n",
    "        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n",
    "        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    # why one is added?\n",
    "\n",
    "    return bounding_boxes.T\n",
    "\n",
    "\n",
    "def detect_faces(image, min_face_size=20.0,\n",
    "                 thresholds=[0.6, 0.7, 0.8],\n",
    "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        min_face_size: a float number.\n",
    "        thresholds: a list of length 3.\n",
    "        nms_thresholds: a list of length 3.\n",
    "\n",
    "    Returns:\n",
    "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
    "        bounding boxes and facial landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD MODELS\n",
    "    pnet = PNet()\n",
    "    rnet = RNet()\n",
    "    onet = ONet()\n",
    "    onet.eval()\n",
    "\n",
    "    # BUILD AN IMAGE PYRAMID\n",
    "    width, height = image.size\n",
    "    min_length = min(height, width)\n",
    "\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "\n",
    "    # scales for scaling the image\n",
    "    scales = []\n",
    "\n",
    "    # scales the image so that\n",
    "    # minimum size that we can detect equals to\n",
    "    # minimum face size that we want to detect\n",
    "    m = min_detection_size/min_face_size\n",
    "    min_length *= m\n",
    "\n",
    "    factor_count = 0\n",
    "    # scales = [0.6, 0.42, 0.30, 0.21, 0.15, 0.10, 0.07, 0.05, 0.03]\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m*factor**factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "\n",
    "    # it will be returned\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # run P-Net on different scales\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # collect boxes (and offsets, and scores) from different scales\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "    # use offsets predicted by pnet to transform bounding boxes\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # shape [n_boxes, 5]\n",
    "\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 2\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            rnet = rnet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    offsets = output[0].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[1].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 3\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "    if len(img_boxes) == 0:\n",
    "        return [], []\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            onet = onet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = onet(img_boxes)\n",
    "    landmarks = output[0].data.cpu().numpy()  # shape [n_boxes, 10]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[2].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    # compute landmark points\n",
    "    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n",
    "    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n",
    "\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2], mode='min') # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    return bounding_boxes, landmarks\n",
    "\n",
    "\n",
    "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
    "    \"\"\"Draw bounding boxes and facial landmarks.\n",
    "\n",
    "    Arguments:\n",
    "        img: an instance of PIL.Image.\n",
    "        bounding_boxes: a float numpy array of shape [n, 5].\n",
    "        facial_landmarks: a float numpy array of shape [n, 10].\n",
    "\n",
    "    Returns:\n",
    "        an instance of PIL.Image.\n",
    "    \"\"\"\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "    # Draw Bounding boxes\n",
    "    for b in bounding_boxes:\n",
    "        draw.rectangle([\n",
    "            (b[0], b[1]), (b[2], b[3])\n",
    "        ], outline='white')\n",
    "\n",
    "    # Draw Facial Landmarks\n",
    "    for p in facial_landmarks:\n",
    "        for i in range(5):\n",
    "            draw.ellipse([\n",
    "                (p[i] - 1.0, p[i + 5] - 1.0),\n",
    "                (p[i] + 1.0, p[i + 5] + 1.0)\n",
    "            ], outline='blue')\n",
    "\n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 Hyperparameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 경로\n",
    "trn_data_dir = './data/CASIA-WebFace'\n",
    "tst_data_dir = './data/lfw'\n",
    "lfw_pair_text = 'lfw_test_part.txt'\n",
    "log_dir = 'log/arcface'\n",
    "weight_dir = 'weight/arcface'\n",
    "\n",
    "# 데이터 관련 세팅\n",
    "batch_size = 16\n",
    "resize = 128\n",
    "crop = 128\n",
    "cls_num=10575\n",
    "gray_scale = True\n",
    "\n",
    "# Hyperparameter\n",
    "feature_dim = 512\n",
    "lr = 0.1  # initial learning rate\n",
    "lr_step = 10000\n",
    "decay_ratio = 0.1\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Setting\n",
    "max_epoch = 200\n",
    "save_every = 10000\n",
    "record_every = 1000\n",
    "display_every = 1000\n",
    "test_num = 6000\n",
    "\n",
    "# GPU가 있을 경우 연산을 GPU에서 하고 없을 경우 CPU에서 진행\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dev = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 객체들을 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크\n",
    "net = ArcFaceNet(feature_dim=feature_dim, \n",
    "                 cls_num=cls_num, \n",
    "                 gray_scale=gray_scale).to(dev)\n",
    "\n",
    "# optimizer 및 learning rate scheduler\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=decay_ratio)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# log 기록\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_similarity(features, test_feature):\n",
    "    sum_ = torch.mm(features, torch.t(test_feature)).squeeze()\n",
    "    norm = torch.norm(features, dim=1) * torch.norm(test_feature)\n",
    "    return sum_ / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bounding_box):\n",
    "    #print(bounding_box)\n",
    "    left, top, right, bottom, _ = bounding_box.astype('int')\n",
    "    cropped_image = TF.crop(image, top, left, bottom-top, right-left)\n",
    "    \n",
    "    #cropped_image.show()\n",
    "\n",
    "    trans_list = []        \n",
    "    trans_list += [T.Grayscale(num_output_channels=1),\n",
    "                   T.Resize((128, 128)),\n",
    "                   T.ToTensor(),\n",
    "                   T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    \n",
    "    transformer = T.Compose(trans_list)\n",
    "    \n",
    "    cropped_image = transformer(cropped_image)\n",
    "    \n",
    "    return cropped_image.unsqueeze(0).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93d0d01b94f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mpil_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mbounding_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mimage_cropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounding_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mface_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_cropped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-15b80ce0b803>\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(image, min_face_size, thresholds, nms_thresholds)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mimg_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mrnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mimg_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_boxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    162\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93d0d01b94f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mpil_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mbounding_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mimage_cropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpil_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounding_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mface_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_cropped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-15b80ce0b803>\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(image, min_face_size, thresholds, nms_thresholds)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mimg_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mrnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mimg_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_boxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    162\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error"
     ]
    }
   ],
   "source": [
    "# fixed_test_pair = fixed_img_list(lfw_pair_text, test_num)\n",
    "\n",
    "load_iters = 290000\n",
    "ckpt_name = 'ckpt_' + str(load_iters) + '.pkl'\n",
    "ckpt_path = os.path.join('weights', ckpt_name)\n",
    "# if torch.cuda.is_available():\n",
    "#     net.load_state_dict(torch.load(ckpt_path))\n",
    "# else:\n",
    "#     net.load_state_dict(torch.load(ckpt_path,map_location='cpu'))\n",
    "net.load_state_dict(torch.load(ckpt_path,map_location='cpu'))\n",
    "net.eval()\n",
    "torch.no_grad()\n",
    "\n",
    "f=open(\"DB.txt\")\n",
    "tmp=f.read()\n",
    "tmp=tmp.split(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "known_face_names = []\n",
    "for i in range(len(tmp)):\n",
    "    if len(tmp[i])>0:\n",
    "        data=tmp[i].split()\n",
    "        known_face_names.append(data[0])\n",
    "        \n",
    "        pil_img = Image.open(data[1])        \n",
    "        bounding_boxes, landmark_points = detect_faces(pil_img)\n",
    "        image_cropped = crop_image(pil_img, bounding_boxes[0])\n",
    "        face_feature = net.extract_feature(image_cropped.to(dev))\n",
    "        \n",
    "        if i==0:\n",
    "            known_face_features=face_feature.clone().detach()\n",
    "        else:\n",
    "            known_face_features=torch.cat((known_face_features, face_feature))\n",
    "\n",
    "\n",
    "unknown_image = Image.open('obama_biden.jpg')\n",
    "bounding_boxes, landmarks = detect_faces(unknown_image)\n",
    "\n",
    "draw = ImageDraw.Draw(unknown_image)\n",
    "\n",
    "# Loop through each face found in the unknown image\n",
    "for box in bounding_boxes:\n",
    "    cropped_image = crop_image(unknown_image, box)\n",
    "    unknown_face_feature=net.extract_feature(cropped_image)\n",
    "\n",
    "    name = \"Unknown\"\n",
    "\n",
    "    # Or instead, use the known face with the smallest distance to the new face\n",
    "    distances = face_similarity(known_face_features, unknown_face_feature)\n",
    "    print(distances)\n",
    "    best_match_index = torch.argmax(distances)\n",
    "    name = known_face_names[best_match_index]\n",
    "    #if matches[best_match_index]:\n",
    "    #    name = known_face_names[best_match_index]\n",
    "    left, top, right, bottom, _ = box.astype('int')\n",
    "\n",
    "    # Draw a box around the face using the Pillow module\n",
    "    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
    "\n",
    "    # Draw a label with a name below the face\n",
    "    text_width, text_height = draw.textsize(name)\n",
    "    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
    "    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
    "    draw.text((left + 6, bottom - text_height +15), \"{0:.2f}\".format(torch.max(distances)), fill=(255, 255, 255, 255))\n",
    "\n",
    "# Remove the drawing library from memory as per the Pillow docs\n",
    "del draw\n",
    "\n",
    "# Display the resulting image\n",
    "display(unknown_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
