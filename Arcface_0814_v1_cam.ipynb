{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ArcFace 실습에 필요한 파이썬 라이브러리를 불러옵니다.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch # pytorch의 tensor와 그와 관련된 기본 연산 등을 지원\n",
    "import torch.nn as nn # 여러 딥러닝 layer와 loss, 함수 등을 클래스 형태로 지원\n",
    "import torch.nn.functional as F # 여러 loss, 함수 등을 function 형태로 지원\n",
    "import torch.optim as optim # 여러 optimizer를 지원\n",
    "\n",
    "from torchvision.datasets import ImageFolder # (img, label) 형태의 데이터셋 구성을 쉽게 할 수 있도록 지원\n",
    "import torchvision.transforms as T # 이미지 전처리를 지원\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils # 여러가지 편리한 기능을 지원 (ex. grid 이미지 만들기 등)\n",
    "import torchvision.models as models # VGG, ResNet 등을 바로 로드할 수 있도록 지원\n",
    "from torch.utils.data import DataLoader # 데이터 로더를 쉽게 만들 수 있도록 지원\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # Tensorflow의 Tensorboard를 지원\n",
    "\n",
    "# GPU 선택\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training에 사용될 데이터를 불러옵니다.\n",
    "  \n",
    "이번 실습에서 사용할 데이터셋은 __CASIA-Webface__ 데이터셋입니다.\n",
    "  \n",
    "  \n",
    "- 총 ID 개수 : 10575  \n",
    "- 총 이미지 개수 : 494414\n",
    "- ID 당 최소 이미지 개수 : 2\n",
    "- ID 당 최대 이미지 개수 : 804\n",
    "- ID 당 평균 이미지 개수 : 46.75\n",
    "- 이미지 크기 : 250 x 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(root, batch_size, resize, crop, gray_scale, shuffle):\n",
    "    '''\n",
    "    목적 : (image, label) 을 반복적으로 로드해주는 데이터 로더 만들기\n",
    "    \n",
    "    인자:\n",
    "    root : 이미지의 루트 디렉토리\n",
    "    batck_size : 배치 사이즈\n",
    "    resize : 이미지 resize 사이즈\n",
    "    crop : 이미지 crop 사이즈\n",
    "    gray_scale : 흑백 변환 여부\n",
    "    shuffle : 무작위 섞음 여부\n",
    "    '''\n",
    "    \n",
    "    ''' Step 1. 이미지 전처리 '''\n",
    "    trans_list = []\n",
    "    \n",
    "    if gray_scale:\n",
    "        trans_list += [T.Grayscale(num_output_channels=1)]\n",
    "        \n",
    "    trans_list += [T.Resize((resize, resize)),\n",
    "                   T.RandomCrop((crop, crop)),\n",
    "                   T.ToTensor()]\n",
    "    \n",
    "    if gray_scale:\n",
    "        trans_list += [T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    else:\n",
    "        trans_list += [T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]\n",
    "        \n",
    "    # Compose에 원하는 전처리의 list를 전달한다.\n",
    "    transformer = T.Compose(trans_list)\n",
    "    \n",
    "    ''' Step 2. Dataset 구성하기 '''\n",
    "    # root - class directory - images 구조에서 (img, label) 꼴을 로드하는 데이터 로더의 구현에 사용 가능\n",
    "    dataset = ImageFolder(root, transform=transformer)\n",
    "    \n",
    "    ''' Step 3. Data loader 만들기'''\n",
    "    dloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    # Tip : 여기서 data loader 뿐만 아니라 class의 개수, data 전체 개수를 같이 반환하면 나중에 편하다.\n",
    "    return dloader, len(dataset.classes), len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace의 모델을 구현합니다.\n",
    "  \n",
    "- Backbone model은 __ResNet-50__ 모델을 사용하도록 하겠습니다.\n",
    "  \n",
    "  \n",
    "# 절차\n",
    "1. ArcFace의 핵심에 해당하는 마지막 fc 의 class 구현\n",
    "2. Pytorch에서 제공해주는 ResNet 을 불러와서 필요한 부분 변경하기\n",
    "3. 1과 2에서 정의된 모듈을 사용하여 ArcFace network 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    '''\n",
    "    목적 : Arc marin 을 포함한 last fc layer의 구현\n",
    "    \n",
    "    인자 :\n",
    "    in_features : feature의 dimension\n",
    "    out_features : class 개수\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        # fc의 parameter 만들기\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        '''\n",
    "        Step 1. cos(theta + m) 계산하기\n",
    "        '''\n",
    "        # cosine : cos(theta)        \n",
    "        # linear(x, W) = x * W^T = [N, in_features] * [in_features, out_features] = [N, out_features]\n",
    "        # [N, out_features] / normalize-> cos dist == cos\n",
    "        # F.normalize는 디폴트가 dim = 1 (줄이고자하는 dimension)\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        \n",
    "        # c^2 + s^2 = 1 \n",
    "        sine = torch.sqrt((1.00000001 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        \n",
    "        # cos(theta + m) = cos(theta) * cos(m) - sin(theta) * sin(m)\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        '''\n",
    "        Step 2. cos(theta + m) 에서 dim=1에 대해 y_i에 해당하는 부분만 남기고 나머지는 cos(theta)로 되돌리기 \n",
    "        '''\n",
    "        one_hot = torch.zeros(cosine.size()).to(dev)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        \n",
    "        '''\n",
    "        Step 3. 이 output이 softmax -> cross entropy loss로 흘러가면 된다.\n",
    "        '''\n",
    "        return output\n",
    "\n",
    "class FeatureNet_50(nn.Module):\n",
    "    '''\n",
    "    목적 : ResNet-50을 이용한 backbone network(feature extractor) 구현 \n",
    "    \n",
    "    인자 :\n",
    "    feature_dim : feature의 dimension\n",
    "    gray_scale : 이미지를 gray scale로 받았는지 여부\n",
    "    '''\n",
    "    def __init__(self, feature_dim, gray_scale=True):\n",
    "        super(FeatureNet_50, self).__init__()\n",
    "        # Pytorch에서 이미 구현되어 있는 resnet-50 불러오기\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 이런식으로 불러온 resnet을 조건에 맞게 변경할 수 있다.\n",
    "        if gray_scale:\n",
    "            resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            \n",
    "        # resnet의 마지막 conv block까지만 남기고 나머지 부분 잘라내기\n",
    "        self.backbone = nn.Sequential(* list(resnet.children())[0:-2])\n",
    "        \n",
    "        # resnet의 마지막 conv block 뒤쪽으로 새로 붙을 layer 들\n",
    "        self.bn_4 = nn.BatchNorm2d(2048)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(2048 * 4 * 4, feature_dim)\n",
    "        self.bn_5 = nn.BatchNorm1d(feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = self.bn_4(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # FC layer를 지나기 전에는 reshape 과정이 필요하다.\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.bn_5(out)\n",
    "        return out\n",
    "\n",
    "class ArcFaceNet(nn.Module):\n",
    "    '''\n",
    "    ArcMarginProduct와 FeatureNet-50 을 결합한 ArcFace 모델의 구현\n",
    "    '''\n",
    "    def __init__(self, feature_dim, cls_num, gray_scale=True):\n",
    "        super(ArcFaceNet, self).__init__()\n",
    "        self.feature_net = FeatureNet_50(feature_dim, gray_scale=gray_scale)\n",
    "        self.classifier = ArcMarginProduct(feature_dim, cls_num)\n",
    "\n",
    "    # 끝까지 Forward 하여 logit을 return\n",
    "    def forward(self, x, label):\n",
    "        out = self.feature_net(x)\n",
    "        out = self.classifier(out, label)\n",
    "        return out\n",
    "    \n",
    "    # Feature만 return\n",
    "    def extract_feature(self, x):\n",
    "        out = self.feature_net(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 사용될 유틸리티 함수들을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(x1, x2):\n",
    "    '''\n",
    "    목적 : 벡터 x1, x2 사이의 Cosine similary를 계산\n",
    "    \n",
    "    인자 :\n",
    "    x1, x2 : 두 벡터\n",
    "    '''\n",
    "    return torch.sum(x1 * x2) / (torch.norm(x1) * torch.norm(x2))\n",
    "\n",
    "def fixed_img_list(lfw_pair_text, test_num):\n",
    "    '''\n",
    "    목적 : 중간 테스트 때 계속 사용될 고정 test 이미지 리스트 생성\n",
    "    \n",
    "    인자:\n",
    "    lfw_pair_text : pair가 만들어진 이미지 경로\n",
    "    test_num : 테스트에 사용할 pair 개수\n",
    "    '''\n",
    "    f = open(lfw_pair_text, 'r')\n",
    "    lines = []\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        lines.append(line)\n",
    "    f.close()\n",
    "\n",
    "    random.shuffle(lines)\n",
    "    lines = lines[:test_num]\n",
    "    return lines\n",
    "\n",
    "def verification(net, pair_list, tst_data_dir, img_size, gray_scale=True):\n",
    "    '''\n",
    "    목적 : Face verification 테스트 수행\n",
    "    \n",
    "    인자:\n",
    "    net : 네트워크\n",
    "    pair_list : pair가 만들어진 이미지 경로의 리스트\n",
    "    tst_data_dir : 테스트 이미지가 있는 루트 디렉토리\n",
    "    img_size : 테스트에 사용할 이미지 사이즈\n",
    "    gray_scale : 흑백으로 변환 여부\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    STEP 1 : 주어진 이미지 pair에서 feature 를 뽑아 similarity, label 리스트 생성\n",
    "    '''\n",
    "    similarities = []\n",
    "    labels = []\n",
    "\n",
    "    # 이미지 전처리\n",
    "    trans_list = []\n",
    "    \n",
    "    #if gray_scale:\n",
    "    #    trans_list += [T.Grayscale(num_output_channels=1)]\n",
    "        \n",
    "    trans_list += [T.CenterCrop((178, 178)),\n",
    "                   T.Resize((img_size, img_size)),\n",
    "                   T.ToTensor()]\n",
    "    \n",
    "    if gray_scale:\n",
    "        trans_list += [T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    else:\n",
    "        trans_list += [T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]\n",
    "    t = T.Compose(trans_list)\n",
    "\n",
    "    # 주어진 모든 이미지 pair에 대해 similarity 계산\n",
    "    net.eval()\n",
    "    with torch.no_grad(): # Test 때 GPU를 사용할 경우 메모리 절약을 위해 torch.no_grad() 내에서 하는 것이 좋다.\n",
    "        for idx, pair in enumerate(pair_list):\n",
    "            # Read paired images\n",
    "            path_1, path_2, label = pair.split(' ')\n",
    "            img_1 = t(Image.open(os.path.join(tst_data_dir, path_1))).unsqueeze(dim=0).to(dev)\n",
    "            img_2 = t(Image.open(os.path.join(tst_data_dir, path_2))).unsqueeze(dim=0).to(dev)\n",
    "            imgs = torch.cat((img_1, img_2), dim=0)\n",
    "\n",
    "            # Extract feature and save\n",
    "            features = net.extract_feature(imgs).cpu()\n",
    "            similarities.append(cos_dist(features[0], features[1]))\n",
    "            labels.append(int(label))\n",
    "            \n",
    "    '''\n",
    "    STEP 2 : similarity와 label로 verification accuracy 측정\n",
    "    '''\n",
    "    best_accr = 0.0\n",
    "    best_th = 0.0\n",
    "\n",
    "    # 각 similarity들이 threshold의 후보가 된다\n",
    "    list_th = similarities\n",
    "    \n",
    "    # list -> tensor\n",
    "    similarities = torch.stack(similarities, dim=0)\n",
    "    labels = torch.ByteTensor(labels)\n",
    "\n",
    "    # 각 threshold 후보에 대해 best accuracy를 측정\n",
    "    for i, th in enumerate(list_th):\n",
    "        pred = (similarities >= th)\n",
    "        correct = (pred == labels)\n",
    "        accr = torch.sum(correct).item() / correct.size(0)\n",
    "\n",
    "        if accr > best_accr:\n",
    "            best_accr = accr\n",
    "            best_th = th.item()\n",
    "\n",
    "    return best_accr, best_th\n",
    "\n",
    "def tsne_visualization(net, tsne_writer, iters, tst_data_dir, img_size=128, cls_num=20, \n",
    "                       min_data_num=10, max_data_num=50):\n",
    "    '''\n",
    "    목적 : 만들어진 고차원 feature를 저차원(2, 3) feature로 변환하여 시각화\n",
    "    \n",
    "    인자 : \n",
    "    net : 네트워크\n",
    "    tsne_writer : 텐서보드 writer 객체\n",
    "    tst_data_dir : 테스트 이미지가 있는 루트 디렉토리\n",
    "    img_size : 이미지 크기\n",
    "    cls_num : 시각화에 사용할 클래스 개수\n",
    "    min_data_num : 클래스 포함되어 있어야 할 최소 이미지 개수\n",
    "    max_data_num : 클래스 포함되어 있어야 할 최대 이미지 개수\n",
    "    '''\n",
    "    \n",
    "    # 보유한 모든 id 에 대해 최소, 최대 이미지 보유 개수를 만족하는 id중 cls_num개만 선정\n",
    "    id_path_list = [os.path.join(tst_data_dir, id) for id in os.listdir(tst_data_dir)]\n",
    "    random.shuffle(id_path_list)\n",
    "    \n",
    "    picked_paths = []\n",
    "    for id_path in id_path_list:\n",
    "        if len(os.listdir(id_path)) > min_data_num and len(os.listdir(id_path)) < max_data_num:\n",
    "            picked_paths.append(id_path)          \n",
    "    picked_paths = picked_paths[:cls_num]\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    t = T.Compose([T.Grayscale(num_output_channels=1),\n",
    "               T.CenterCrop((178, 178)),\n",
    "               T.Resize((img_size, img_size)),\n",
    "               T.ToTensor(),\n",
    "               T.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "    \n",
    "    # 선정된 id들의 모든 이미지를 읽고 이미지에 대한 id를 함께 저장\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for id, id_path in enumerate(picked_paths):\n",
    "        for img_name in os.listdir(id_path):\n",
    "            img_path = os.path.join(id_path, img_name)\n",
    "            img = t(Image.open(img_path)).unsqueeze(dim=0)\n",
    "            imgs.append(img)\n",
    "            labels.append(id)\n",
    "            \n",
    "    # 모든 이미지들에 대한 feature 뽑기\n",
    "    # features = (N, feature_dim) / labels = (N, )\n",
    "    imgs = torch.cat(imgs, dim=0).to(dev)\n",
    "    with torch.no_grad():\n",
    "        features = net.extract_feature(imgs)\n",
    "        \n",
    "    # T-sne를 이용한 시각화\n",
    "    tsne_writer.add_embedding(features, metadata=labels, global_step=iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from box_utils import nms, calibrate_box, get_image_boxes, convert_to_square, _preprocess\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, c, h, w].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch_size, c*h*w].\n",
    "        \"\"\"\n",
    "\n",
    "        # without this pretrained model isn't working\n",
    "        x = x.transpose(3, 2).contiguous()\n",
    "\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class PNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "        # suppose we have input with size HxW, then\n",
    "        # after first layer: H - 2,\n",
    "        # after pool: ceil((H - 2)/2),\n",
    "        # after second conv: ceil((H - 2)/2) - 2,\n",
    "        # after last conv: ceil((H - 2)/2) - 4,\n",
    "        # and the same for W\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 10, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(10)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(10, 16, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(16)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(16, 32, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(32))\n",
    "        ]))\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
    "\n",
    "        weights = np.load('weights/pnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
    "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv4_1(x)\n",
    "        b = self.conv4_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class RNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 28, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(28)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(28, 48, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(48)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(48, 64, 2, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv4', nn.Linear(576, 128)),\n",
    "            ('prelu4', nn.PReLU(128))\n",
    "        ]))\n",
    "\n",
    "        self.conv5_1 = nn.Linear(128, 2)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        weights = np.load('weights/rnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv5_1(x)\n",
    "        b = self.conv5_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a\n",
    "\n",
    "\n",
    "class ONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(32)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(64)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
    "            ('prelu4', nn.PReLU(128)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv5', nn.Linear(1152, 256)),\n",
    "            ('drop5', nn.Dropout(0.25)),\n",
    "            ('prelu5', nn.PReLU(256)),\n",
    "        ]))\n",
    "\n",
    "        self.conv6_1 = nn.Linear(256, 2)\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "\n",
    "        weights = np.load('weights/onet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            c: a float tensor with shape [batch_size, 10].\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv6_1(x)\n",
    "        b = self.conv6_2(x) # Bounding Box Regression\n",
    "        c = self.conv6_3(x) # Face Landmark Detection\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return c, b, a\n",
    "\n",
    "\n",
    "def run_first_stage(image, net, scale, threshold):\n",
    "    \"\"\"Run P-Net, generate bounding boxes, and do NMS.\n",
    "\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        net: an instance of pytorch's nn.Module, P-Net.\n",
    "        scale: a float number,\n",
    "            scale width and height of the image by this number.\n",
    "        threshold: a float number,\n",
    "            threshold on the probability of a face when generating\n",
    "            bounding boxes from predictions of the net.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9],\n",
    "            bounding boxes with scores and offsets (4 + 1 + 4).\n",
    "    \"\"\"\n",
    "\n",
    "    # scale the image and convert it to a float array\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n",
    "    img = image.resize((int(sw), int(sh)), Image.BILINEAR)\n",
    "    img = np.asarray(img, 'float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = Variable(torch.FloatTensor(_preprocess(img)))\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            net = net.cuda()\n",
    "    output = net(img)\n",
    "    probs = output[1].data.cpu().numpy()[0, 1, :, :]\n",
    "    offsets = output[0].data.cpu().numpy()\n",
    "    # probs: probability of a face at each sliding window\n",
    "    # offsets: transformations to true bounding boxes\n",
    "\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \"\"\"Generate bounding boxes at places\n",
    "    where there is probably a face.\n",
    "\n",
    "    Arguments:\n",
    "        probs: a float numpy array of shape [n, m].\n",
    "        offsets: a float numpy array of shape [1, 4, n, m].\n",
    "        scale: a float number,\n",
    "            width and height of the image were scaled by this number.\n",
    "        threshold: a float number.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9]\n",
    "    \"\"\"\n",
    "\n",
    "    # applying P-Net is equivalent, in some sense, to\n",
    "    # moving 12x12 window with stride 2\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    # indices of boxes where there is probably a face\n",
    "    inds = np.where(probs > threshold)\n",
    "\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # transformations of bounding boxes\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    # they are defined as:\n",
    "    # w = x2 - x1 + 1\n",
    "    # h = y2 - y1 + 1\n",
    "    # x1_true = x1 + tx1*w\n",
    "    # x2_true = x2 + tx2*w\n",
    "    # y1_true = y1 + ty1*h\n",
    "    # y2_true = y2 + ty2*h\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "\n",
    "    # P-Net is applied to scaled images\n",
    "    # so we need to rescale bounding boxes back\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride*inds[1] + 1.0)/scale),\n",
    "        np.round((stride*inds[0] + 1.0)/scale),\n",
    "        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n",
    "        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    # why one is added?\n",
    "\n",
    "    return bounding_boxes.T\n",
    "\n",
    "\n",
    "def detect_faces(image, min_face_size=20.0,\n",
    "                 thresholds=[0.6, 0.7, 0.8],\n",
    "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        min_face_size: a float number.\n",
    "        thresholds: a list of length 3.\n",
    "        nms_thresholds: a list of length 3.\n",
    "\n",
    "    Returns:\n",
    "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
    "        bounding boxes and facial landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD MODELS\n",
    "    pnet = PNet()\n",
    "    rnet = RNet()\n",
    "    onet = ONet()\n",
    "    onet.eval()\n",
    "\n",
    "    # BUILD AN IMAGE PYRAMID\n",
    "    width, height = image.size\n",
    "    min_length = min(height, width)\n",
    "\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "\n",
    "    # scales for scaling the image\n",
    "    scales = []\n",
    "\n",
    "    # scales the image so that\n",
    "    # minimum size that we can detect equals to\n",
    "    # minimum face size that we want to detect\n",
    "    m = min_detection_size/min_face_size\n",
    "    min_length *= m\n",
    "\n",
    "    factor_count = 0\n",
    "    # scales = [0.6, 0.42, 0.30, 0.21, 0.15, 0.10, 0.07, 0.05, 0.03]\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m*factor**factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "\n",
    "    # it will be returned\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # run P-Net on different scales\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # collect boxes (and offsets, and scores) from different scales\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "    # use offsets predicted by pnet to transform bounding boxes\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # shape [n_boxes, 5]\n",
    "\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 2\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            rnet = rnet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    offsets = output[0].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[1].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 3\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "    if len(img_boxes) == 0:\n",
    "        return [], []\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            onet = onet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = onet(img_boxes)\n",
    "    landmarks = output[0].data.cpu().numpy()  # shape [n_boxes, 10]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[2].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    # compute landmark points\n",
    "    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n",
    "    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n",
    "\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2], mode='min') # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    return bounding_boxes, landmarks\n",
    "\n",
    "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
    "    \"\"\"Draw bounding boxes and facial landmarks.\n",
    "\n",
    "    Arguments:\n",
    "        img: an instance of PIL.Image.\n",
    "        bounding_boxes: a float numpy array of shape [n, 5].\n",
    "        facial_landmarks: a float numpy array of shape [n, 10].\n",
    "\n",
    "    Returns:\n",
    "        an instance of PIL.Image.\n",
    "    \"\"\"\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "    # Draw Bounding boxes\n",
    "    for b in bounding_boxes:\n",
    "        draw.rectangle([\n",
    "            (b[0], b[1]), (b[2], b[3])\n",
    "        ], outline='white')\n",
    "\n",
    "    # Draw Facial Landmarks\n",
    "    for p in facial_landmarks:\n",
    "        for i in range(5):\n",
    "            draw.ellipse([\n",
    "                (p[i] - 1.0, p[i + 5] - 1.0),\n",
    "                (p[i] + 1.0, p[i + 5] + 1.0)\n",
    "            ], outline='blue')\n",
    "\n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 Hyperparameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 경로\n",
    "trn_data_dir = './data/CASIA-WebFace'\n",
    "tst_data_dir = './data/lfw'\n",
    "lfw_pair_text = 'lfw_test_part.txt'\n",
    "log_dir = 'log/arcface'\n",
    "weight_dir = 'weight/arcface'\n",
    "\n",
    "# 데이터 관련 세팅\n",
    "batch_size = 16\n",
    "resize = 128\n",
    "crop = 128\n",
    "cls_num=10575\n",
    "gray_scale = True\n",
    "\n",
    "# Hyperparameter\n",
    "feature_dim = 512\n",
    "lr = 0.1  # initial learning rate\n",
    "lr_step = 10000\n",
    "decay_ratio = 0.1\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Setting\n",
    "max_epoch = 200\n",
    "save_every = 10000\n",
    "record_every = 1000\n",
    "display_every = 1000\n",
    "test_num = 6000\n",
    "\n",
    "# GPU가 있을 경우 연산을 GPU에서 하고 없을 경우 CPU에서 진행\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 디렉토리가 없을 시 생성\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습에 필요한 객체들을 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크\n",
    "net = ArcFaceNet(feature_dim=feature_dim, \n",
    "                 cls_num=cls_num, \n",
    "                 gray_scale=gray_scale).to(dev)\n",
    "\n",
    "# optimizer 및 learning rate scheduler\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=decay_ratio)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# log 기록\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 현재 epoch와 현재까지의 전체 iteration\\nepoch = 0\\ntotal_iters = 0\\n\\n# Best accuracy와 그 때의 iteration\\nbest_accr = 0.0\\nbest_iter = 0\\n\\n# 중간 test에 사용할 고정 테스트 이미지 pair\\nfixed_test_pair = fixed_img_list(lfw_pair_text, test_num)\\n\\n# 1 epoch 당 iterationㅇ,; 수\\niter_per_epoch = (data_num // batch_size) + 1\\n\\n# Training 시작\\nwhile(epoch < max_epoch):\\n    for iters, (img, label) in enumerate(dloader):\\n        # 매 iteration 마다 필요한 preprocess\\n        net.train()\\n        scheduler.step()\\n        optimizer.zero_grad()\\n\\n        # 이미지, label을 로드하여 forward\\n        img, label = img.to(dev), label.to(dev)\\n        logit = net(img, label)\\n        loss = criterion(logit, label)\\n\\n        # Bacpropagation 및 update\\n        loss.backward()\\n        optimizer.step()\\n\\n        # 현재의 총 iteration 계산\\n        total_iters = (iter_per_epoch * epoch) + iters\\n\\n        # 지금이 Record를 해야할 iteration일 경우\\n        if total_iters % record_every == 0:\\n            net.eval()\\n            \\n            # 중간 테스트\\n            accr, th = verification(net, fixed_test_pair, tst_data_dir, resize, gray_scale)\\n\\n            # Tensorboard에 기록\\n            writer.add_scalar(\\'loss\\', loss.item(), total_iters)\\n            writer.add_scalar(\\'accr\\', accr, total_iters)\\n            writer.add_scalar(\\'lr\\', scheduler.get_lr()[0], total_iters)\\n            \\n            # Best accuracy 체크\\n            if accr > best_accr:\\n                best_accr = accr\\n                best_iter = total_iters\\n\\n        # 지금이 중간 결과를 보여줘야할 iteration일 경우\\n        if total_iters % display_every == 0:\\n            print(time.strftime(\"%Y-%m-%d %H:%M\" + \":00\"))\\n            print(\\'* [Total iters : %d (epoch : %d / iter : %d)] => loss : %f, best_accr : %f(iter : %d)\\n\\'                  %(total_iters, epoch, iters, loss.item(), best_accr, best_iter))\\n\\n        # 지금이 모델을 저장해야할 iteration일 경우\\n        if total_iters % save_every == 0:\\n            file_name = \\'ckpt_\\' + str(total_iters) + \\'.pkl\\'\\n            path_ckpt = os.path.join(weight_dir, file_name)\\n            torch.save(net.state_dict(), path_ckpt)\\n\\n    # 모든 데이터를 다 돌면 epoch 증가\\n    epoch = epoch + 1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 현재 epoch와 현재까지의 전체 iteration\n",
    "epoch = 0\n",
    "total_iters = 0\n",
    "\n",
    "# Best accuracy와 그 때의 iteration\n",
    "best_accr = 0.0\n",
    "best_iter = 0\n",
    "\n",
    "# 중간 test에 사용할 고정 테스트 이미지 pair\n",
    "fixed_test_pair = fixed_img_list(lfw_pair_text, test_num)\n",
    "\n",
    "# 1 epoch 당 iterationㅇ,; 수\n",
    "iter_per_epoch = (data_num // batch_size) + 1\n",
    "\n",
    "# Training 시작\n",
    "while(epoch < max_epoch):\n",
    "    for iters, (img, label) in enumerate(dloader):\n",
    "        # 매 iteration 마다 필요한 preprocess\n",
    "        net.train()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 이미지, label을 로드하여 forward\n",
    "        img, label = img.to(dev), label.to(dev)\n",
    "        logit = net(img, label)\n",
    "        loss = criterion(logit, label)\n",
    "\n",
    "        # Bacpropagation 및 update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 현재의 총 iteration 계산\n",
    "        total_iters = (iter_per_epoch * epoch) + iters\n",
    "\n",
    "        # 지금이 Record를 해야할 iteration일 경우\n",
    "        if total_iters % record_every == 0:\n",
    "            net.eval()\n",
    "            \n",
    "            # 중간 테스트\n",
    "            accr, th = verification(net, fixed_test_pair, tst_data_dir, resize, gray_scale)\n",
    "\n",
    "            # Tensorboard에 기록\n",
    "            writer.add_scalar('loss', loss.item(), total_iters)\n",
    "            writer.add_scalar('accr', accr, total_iters)\n",
    "            writer.add_scalar('lr', scheduler.get_lr()[0], total_iters)\n",
    "            \n",
    "            # Best accuracy 체크\n",
    "            if accr > best_accr:\n",
    "                best_accr = accr\n",
    "                best_iter = total_iters\n",
    "\n",
    "        # 지금이 중간 결과를 보여줘야할 iteration일 경우\n",
    "        if total_iters % display_every == 0:\n",
    "            print(time.strftime(\"%Y-%m-%d %H:%M\" + \":00\"))\n",
    "            print('* [Total iters : %d (epoch : %d / iter : %d)] => loss : %f, best_accr : %f(iter : %d)\\n'\\\n",
    "                  %(total_iters, epoch, iters, loss.item(), best_accr, best_iter))\n",
    "\n",
    "        # 지금이 모델을 저장해야할 iteration일 경우\n",
    "        if total_iters % save_every == 0:\n",
    "            file_name = 'ckpt_' + str(total_iters) + '.pkl'\n",
    "            path_ckpt = os.path.join(weight_dir, file_name)\n",
    "            torch.save(net.state_dict(), path_ckpt)\n",
    "\n",
    "    # 모든 데이터를 다 돌면 epoch 증가\n",
    "    epoch = epoch + 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cos_dist(x1, x2):\n",
    "#     return torch.sum(x1 * x2) / (torch.norm(x1) * torch.norm(x2))\n",
    "\n",
    "def face_similarity(features, test_feature):\n",
    "    sum_ = torch.mm(features, torch.t(test_feature)).squeeze()\n",
    "    norm = torch.norm(features, dim=1) * torch.norm(test_feature)\n",
    "    return sum_ / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bound_box(image):\n",
    "    face_recognition.face_locations(image)\n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bounding_box):\n",
    "    left, top, right, bottom, _ = bounding_box.astype('int')\n",
    "    cropped_image = TF.crop(image, top, left, bottom-top, right-left)\n",
    "    \n",
    "    #cropped_image.show()\n",
    "\n",
    "    trans_list = []        \n",
    "    trans_list += [T.Grayscale(num_output_channels=1),\n",
    "                   T.Resize((128, 128)),\n",
    "                   T.ToTensor(),\n",
    "                   T.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "    \n",
    "    transformer = T.Compose(trans_list)\n",
    "    \n",
    "    cropped_image = transformer(cropped_image)\n",
    "    \n",
    "    return cropped_image.unsqueeze(0).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned feature for 2 images.\n"
     ]
    }
   ],
   "source": [
    "# fixed_test_pair = fixed_img_list(lfw_pair_text, test_num)\n",
    "\n",
    "load_iters = 200000\n",
    "ckpt_name = 'ckpt_' + str(load_iters) + '.pkl'\n",
    "ckpt_path = os.path.join('weights/arcface_pretrained', ckpt_name)\n",
    "net.load_state_dict(torch.load(ckpt_path))\n",
    "net.eval()\n",
    "torch.no_grad()\n",
    "# This is an example of running face recognition on a single image\n",
    "# and drawing a box around each person that was identified.\n",
    "obama_image = Image.open('obama.jpg')\n",
    "bounding_boxes, landmarks = detect_faces(obama_image)\n",
    "obama_image_cropped = crop_image(obama_image, bounding_boxes[0])\n",
    "\n",
    "biden_image = Image.open('biden.jpg')\n",
    "bounding_boxes, landmarks = detect_faces(biden_image)\n",
    "biden_image_cropped = crop_image(biden_image, bounding_boxes[0])\n",
    "\n",
    "obama_face_feature = net.extract_feature(obama_image_cropped.to(dev))\n",
    "biden_face_feature = net.extract_feature(biden_image_cropped.to(dev))\n",
    "\n",
    "known_face_features = torch.cat((obama_face_feature, biden_face_feature))\n",
    "\n",
    "known_face_names = [\n",
    "    \"Barack Obama\",\n",
    "    \"Joe Biden\"\n",
    "]\n",
    "print('Learned feature for', len(known_face_features), 'images.')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    _, frame = video_capture.read()\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    cv2_im = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    pil_im = Image.fromarray(cv2_im)\n",
    "    \n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    bounding_boxes, landmarks = detect_faces(pil_im)\n",
    "    for box in bounding_boxes:\n",
    "        cropped_image = crop_image(pil_im, box)\n",
    "        #print(box)\n",
    "        unknown_face_feature=net.extract_feature(cropped_image)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Or instead, use the known face with the smallest distance to the new face\n",
    "        distances = face_similarity(known_face_features, unknown_face_feature)\n",
    "        #print(distances)\n",
    "        best_match_index = torch.argmax(distances)\n",
    "        if torch.max(distances)>0.3:\n",
    "            name = known_face_names[best_match_index]\n",
    "\n",
    "        left, top, right, bottom, _ = box.astype('int')\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        cv2.putText(frame, str(\"{0:.2f}\".format(torch.max(distances))), (left + 6, bottom + 30), font, 1.0, (255, 255, 255), 1)\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
